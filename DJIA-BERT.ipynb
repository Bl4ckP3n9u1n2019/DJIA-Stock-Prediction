{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596574268203",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinary(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinary, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "\n",
    "        return linear_output\n",
    "\n",
    "model = BertBinary()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "data = pd.read_csv(r'Data\\News_DJIA.csv')\n",
    "\n",
    "# combine titles \n",
    "title_cols = list(data.columns[2:24]) # using Top 22 titles due to BERTs max sequence length of 512\n",
    "data['News'] = data[title_cols].agg(' '.join, axis = 1)\n",
    "\n",
    "# remove, quotes and b, from news title cols \n",
    "def clean_titles(titles):\n",
    "    titles = re.sub('b[(\\')]','',titles)\n",
    "    titles = re.sub('b[(\\\")]','',titles)\n",
    "    titles = re.sub(\"\\'\",'',titles)\n",
    "    return titles\n",
    "\n",
    "data['News'] = data.apply(lambda x: clean_titles(x['News']), axis = 1)\n",
    "\n",
    "# drop un-used cols\n",
    "data = data.drop(data.columns[2:27], axis = 1)\n",
    "\n",
    "# Date to datetime object\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and test\n",
    "split = dt.datetime(2015,1,1,0,0,0)\n",
    "train = data[data.Date <= split]\n",
    "test = data[data.Date > split]\n",
    "X_train, y_train = np.array(train['News']),np.array(train['Label'])\n",
    "X_test, y_test = np.array(test['News']),np.array(test['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize, ID, mask, and pad text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "max_len = 512 # hard limit for BERT\n",
    "\n",
    "# tokenize text\n",
    "def tokenize_ID(text):\n",
    "        \n",
    "    # tokenize and add clasification and seperation tokens\n",
    "    text_tokenized = tokenizer.tokenize(text)\n",
    "    text_tokenized.insert(0,'[CLS]')\n",
    "    if len(text_tokenized) >= max_len:\n",
    "        text_tokenized.insert(max_len -1,'[SEP]')\n",
    "    else:\n",
    "        text_tokenized.append('[SEP]')\n",
    "    \n",
    "    # convert tokens to IDs\n",
    "    IDs = tokenizer.convert_tokens_to_ids(text_tokenized)\n",
    "\n",
    "    return IDs\n",
    "\n",
    "X_train = [tokenize_ID(text) for text in X_train]\n",
    "X_test = [tokenize_ID(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad/trim IDs\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, truncating=\"post\", padding=\"post\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat masks\n",
    "train_masks = [[float(i > 0) for i in ii] for ii in X_train]\n",
    "test_masks = [[float(i > 0) for i in ii] for ii in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch data\n",
    "batch_size = 4\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train),torch.tensor(train_masks), torch.from_numpy(y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test),torch.tensor(test_masks), torch.from_numpy(y_test))\n",
    "\n",
    "# create data loaders\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training functions and parameters\n",
    "optimizer = Adam(params = model.parameters(), lr = 1e-4)\n",
    "loss_func = nn.CrossEntropyLoss(weight=None)\n",
    "EPOCHS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch_num in range(EPOCHS):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # itterate through train data\n",
    "    for step_num, batch_data in enumerate(train_loader):\n",
    "        token_ids,masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "        # https://github.com/huggingface/transformers/issues/2952\n",
    "        token_ids = token_ids.type(torch.LongTensor)\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        token_ids = token_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # end bug fix\n",
    "\n",
    "        model.zero_grad()\n",
    "        logits = model(token_ids, masks)  \n",
    "        batch_loss = loss_func(logits, labels)\n",
    "        train_loss += batch_loss.item()      \n",
    "        batch_loss.backward()\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    predictions = []\n",
    "    solutions = []\n",
    "\n",
    "    # itterate through test data\n",
    "    with torch.no_grad():\n",
    "        for step_num_eval, batch_data in enumerate(test_loader):\n",
    "            token_ids,masks, labels = tuple(t.to(device) for t in batch_data)\n",
    "\n",
    "            # https://github.com/huggingface/transformers/issues/2952\n",
    "            token_ids = token_ids.type(torch.LongTensor)\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            token_ids = token_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # end bug fix\n",
    "\n",
    "            logits = model(token_ids, masks)  \n",
    "            batch_loss = loss_func(logits, labels)\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "            for pred in logits.cpu().detach().numpy():\n",
    "                predictions.append(pred.tolist())\n",
    "            for sol in labels.cpu().detach().numpy():\n",
    "                solutions.append(sol.item())\n",
    "\n",
    "        predictions_ = [x.index(max(x)) for x in predictions]\n",
    "        correct = predictions_ == np.array(solutions)\n",
    "        accuracy = correct.tolist().count(True)/len(y_test)\n",
    "\n",
    "    print('Epoch:{}, loss: {}, Accuracy: {}'.format(epoch_num, val_loss/(step_num_eval+1),accuracy))\n"
   ]
  }
 ]
}